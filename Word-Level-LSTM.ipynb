{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note: sequence generating code is taken from machinelearningmastery.com\n",
    "## the model was written by us using keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matt\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "import keras\n",
    "import random\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from Utility import Utility\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 layer LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "util = Utility()\n",
    "#sonnets = util.get_collab()\n",
    "sonnets = util.get_other('data/limericks.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonnetString = []\n",
    "for i in sonnets:\n",
    "    sA = sonnets[i]\n",
    "    sonnetString.append(\"\")\n",
    "    for k in sA:\n",
    "        for j in k:\n",
    "            sonnetString[-1] += j + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences in sonnet: 27503\n"
     ]
    }
   ],
   "source": [
    "sequences = []\n",
    "bigdata = ' '.join(sonnetString)\n",
    "\n",
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([bigdata])\n",
    "\n",
    "sequences = []\n",
    "for data in sonnetString:\n",
    "    encoded = tokenizer.texts_to_sequences([data])[0]\n",
    "    # retrieve vocabulary size\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    # encode 3 words -> 1 word\n",
    "    \n",
    "    for i in range(3, len(encoded)):\n",
    "        sequence = encoded[i-3:i+1]\n",
    "        sequences.append(sequence)\n",
    "    \n",
    "print('Total Sequences in sonnet: %d' % len(sequences))\n",
    "max_length = max([len(seq) for seq in sequences])\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 3, 10)             52470     \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 3, 75)             25800     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 3, 75)             0         \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 75)                45300     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 5247)              398772    \n",
      "=================================================================\n",
      "Total params: 522,342\n",
      "Trainable params: 522,342\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# split into input and output elements\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1],sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=max_length-1))\n",
    "model.add(LSTM(75, return_sequences=True))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(LSTM(75))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "model.load_weights(\"check_lim/word-201.hdf5\")\n",
    "# compile network\n",
    "adam = keras.optimizers.Adam(lr=1e-3)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"word-{epoch:03d}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "model.fit(X, y, epochs=999, batch_size=128,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds+0.001) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    if(np.sum(exp_preds) != 0):\n",
    "        preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, max_length, seed_text, n_words):\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pre-pad sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "        # predict probabilities for each word\n",
    "        prediction = model.predict(encoded, verbose=0)[0]\n",
    "        yhat = sample(prediction, 0.6)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are training on actual words here, so can look up their syllable counts from \n",
    "# the dictionary, to perform some automatic post-processing\n",
    "\n",
    "# Add in line breaks, punctuation, capitalization and truncation where appropriate\n",
    "def post_process(sequence, max_syll=10, max_lines = 14):\n",
    "    # Load syllable dictionary\n",
    "    d_int = {}\n",
    "    with open('data/syllable_dict_all.txt') as f:\n",
    "        for line in f:\n",
    "            arr = line.split()\n",
    "            if arr[0] not in d_int.keys():\n",
    "                d_int[arr[0]] = ' '.join(arr[1:])\n",
    "\n",
    "    output_words = sequence.split()\n",
    "    output_sonnet = output_words[0].capitalize()\n",
    "    syll_counter = int(d_int[output_words[0]][-1])\n",
    "    line_counter = 1\n",
    "\n",
    "    for j in range(1,len(output_words)):\n",
    "        syll_counter += int(d_int[output_words[j]][-1])\n",
    "        if syll_counter > max_syll:\n",
    "            # Add a sentence after each 4th line/the final line\n",
    "            # Randomly add commas at ends of other lines\n",
    "            if (line_counter % 4 == 0) or (line_counter == max_lines):\n",
    "                output_sonnet += '.\\n'\n",
    "            else: \n",
    "                if random.randint(0,1) == 1:\n",
    "                    output_sonnet += ',\\n'\n",
    "                else:\n",
    "                    output_sonnet += '\\n'\n",
    "\n",
    "            # Roll over to next line, or stop if we have enough lines\n",
    "            line_counter += 1\n",
    "            if line_counter <= max_lines:           \n",
    "                output_sonnet += output_words[j].capitalize()\n",
    "            else:\n",
    "                break\n",
    "            syll_counter = 0\n",
    "        else:\n",
    "            # Capitalize i and i'll\n",
    "            if (output_words[j] == 'i') or (output_words[j] == \"i'll\"):\n",
    "                output_sonnet += ' ' + output_words[j].capitalize()\n",
    "            else:\n",
    "                output_sonnet += ' ' + output_words[j]\n",
    "\n",
    "    return output_sonnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deez nuts to toronto day his hairy\n",
      "But able and ill eager girls' to large,\n",
      "Profane strong o wench's lies forget'st snatch they to\n",
      "Blowed the rest her would delight as he ploughed through.\n",
      "The shite of cunts it was fucking and quite,\n",
      "Kneeling men to dearest painting strength to in\n",
      "Posterity he not soundless would par I say thee,\n",
      "Maketh be or quickly frighten and from in.\n",
      "Wondrous feathers by arse excess life some fist and,\n",
      "Swain a cane said was almost for long with\n",
      "Interrupting to crowned trim to every young\n",
      "Enthusiasm of rio of ten a thing but too.\n",
      "Striver disadvantage they of reason her pride\n",
      "Sharp and the spring of day and told my limbs and.\n",
      "\n",
      "Deez nuts of sailors' world either did\n",
      "Party eclipsed the name burthen to fuck the piece,\n",
      "Of the treasure of the spring and foison of,\n",
      "The year of imitated shall keep on his.\n",
      "Rain of some that eye's thing should went on than\n",
      "Compiled my praises strong and rest to black sensual,\n",
      "Comfort those be not is her will tilted every\n",
      "Time and make the lay of thee that friend.\n",
      "Vermilion sympathized to beauty's lass the damn as,\n",
      "A fat bottomed feed'st of a piece and be in\n",
      "Grecian remembrance disdaineth full her pride to,\n",
      "Yale and was getting as a hole when his crime.\n",
      "O I swift reigned wench o'er keep'st the orphans and\n",
      "Fiery fruit urgin' hours sweetness alack.\n",
      "\n",
      "Deez nuts bestow to sorrow\n",
      "Whatsoever my other wriggle than right made wills but\n",
      "He to pleasing note bear sing when I being\n",
      "Wood's gems and more effect I see she run'st on.\n",
      "Bide tough if an filching thing why still this ill\n",
      "So gorgon his vendor had her merely had,\n",
      "A bastard long is still coral but that\n",
      "Pretty the must of go' with the balcony wop.\n",
      "Let revenues and approve whatsoever,\n",
      "The sight vibrant brand of isis crawls\n",
      "Unprovident ended falsely my hearts and do slender\n",
      "Birth leave in thereby utensil you seen me.\n",
      "Sweet humour canoein' of dead thou strong soil\n",
      "Accuse for in thy picture and my love hath they.\n",
      "\n",
      "Deez nuts to you pictured glory o to\n",
      "Love my self was breed for he like invasion,\n",
      "Spunk past in minister conspire back to just,\n",
      "Lives but servant's about very hard could so.\n",
      "Fond in all the finger of a trice the\n",
      "Innermost parts of a wench of beaconsfield at it,\n",
      "You was expressed to ladies that when churl lease,\n",
      "You like my sake provide thou promise honour.\n",
      "And adore you to give ruinate sickly\n",
      "And repair please thy mistress' paper within,\n",
      "Feeble car the intelligence wouldn't snook of\n",
      "Bier to joy with fair as evil suck invent.\n",
      "This both thee and occurred to a man of,\n",
      "Watchman sensitivity in docking church was.\n",
      "\n",
      "Deez nuts to my love angry for his,\n",
      "Prescriptions are not kept as mine which thee thou wilt time's,\n",
      "Footed so brand to lace it lecherous quick bore\n",
      "Tossed first the same do somebody acted and.\n",
      "My own self ask or warts captain vanishing\n",
      "While prehensile or banks they not then with warmed\n",
      "Nice not the course should dart nor former pud your,\n",
      "End eye's world account to will that you ever.\n",
      "Seem my self doth mark me how even I,\n",
      "Hindmost cot awfully used I long hold\n",
      "Heavily deserved it in hapless don daily hadst\n",
      "Up men to sweetest elephant's mclennin.\n",
      "And and non rubber minimis mcguzzum\n",
      "Addeth striking filching begin with love clyster.\n",
      "\n",
      "Deez nuts of girls throned and wave wards and,\n",
      "Your engraft it minds of age's divine and\n",
      "Lacked tongues of his sweet deaths are sweetest odours\n",
      "Made and from his dazzle to fuck to was nice.\n",
      "As long as flustered and he cried buried\n",
      "Strike thy receiv'st and pell thine polished,\n",
      "Informer to be lays my idolatry o no,\n",
      "Self me in my fragrant thoughts I simply by.\n",
      "April stains when he'd fucked a twitcher was jap stain,\n",
      "Phallus salutation witness to sportive\n",
      "Mansion of to vices girls as not stuff doth clears,\n",
      "As thine self kind to schwartz it he getting the.\n",
      "Wife named wife whose marriage as eternity,\n",
      "Of becoming in you're he'd rolling titties.\n",
      "\n",
      "Deez nuts to you tickled my vows artful,\n",
      "Reap before sang shallowest borne hath steadfastly,\n",
      "Was more distinguishing crawl chancres he the\n",
      "Plaudits and you dignified detroit other.\n",
      "Angel speaking and been faith torn the cup trained in,\n",
      "A bride a wife told my organs screw crowned\n",
      "Delights her memory debarred gift,\n",
      "Triangular to having crackers from the mountain it.\n",
      "The sea think the executor to be self,\n",
      "Thou quid the herd and summer's green spends my herd,\n",
      "And night sometimes down steuben vilest man she,\n",
      "Had crossed her unset but got fuck at penis.\n",
      "He told terribly mack out was fucking as,\n",
      "Speechless stood the balls but hairier of a.\n",
      "\n",
      "Deez nuts of this praise as motion quote that\n",
      "Thy bible costly heart eked so a living\n",
      "Brow before your quest to prove extreme beauty,\n",
      "This my bootless bearer and character but.\n",
      "Daily may when come doth line be disposed to trust\n",
      "Eyes mock thy praise and so gift and boldness\n",
      "Noted prove blushing legs to be show the rud'st he\n",
      "Gentlest sight the most account and memory thy.\n",
      "Love pardon to wrinkles foes five heaven are,\n",
      "You are loose ah is gusts to brougham huge,\n",
      "Complained imitate and gentle pupil wits but\n",
      "In a hottentot gal said endeared north.\n",
      "Of durand he was fond but said a very\n",
      "Fossil he haughty it beguiled with self.\n",
      "\n",
      "Deez nuts to you am none not not lady's\n",
      "Chief time and stand time's surgical vagina\n",
      "To plagues he strengthen calls around the praise wish,\n",
      "Her life and rover and fee in a cold.\n",
      "Valley fountain of my ground can fuck my self\n",
      "Another quiet find thy havre sickle's external\n",
      "Eye eating commits and roughly found of my\n",
      "Mean little dong named beasts blunt remove was inch.\n",
      "Of a blooey easy how a time past whose,\n",
      "Graves walks are the plug of bollocks grew are,\n",
      "Nutrition and diversion diet urge quite to,\n",
      "Prevent so translated harvest nose good to niece.\n",
      "Would told diverting to pruritus her seal,\n",
      "Image strained anew andante directly.\n",
      "\n",
      "Deez nuts to odour thus maketh I give,\n",
      "Suns when the world of I of be troy\n",
      "Reproving dearths in newer quality had to so,\n",
      "Long and years aged air suck touched ugly who.\n",
      "Vanishing 'winsome' shit famine do can't it come it,\n",
      "With the world that wail this my erred and to this,\n",
      "Feil sick shitter to eat or the old world get,\n",
      "Them hence invasion war still atavistic.\n",
      "To thee sped column to keen gazeth pastor\n",
      "In with feeling some refined lines but for\n",
      "Me my self indeed beated and chopt with tanned\n",
      "Antiquity mine grieve all lease legged frantic to hues.\n",
      "And saw kent the tool by a inch fierce spot for\n",
      "Lewd to days to flourish thou hast disposed and.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for j in range(10):\n",
    "    sonnet_raw = generate_seq(model, tokenizer, max_length-1, \"deez nuts\", 150)\n",
    "    print(post_process(sonnet_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
