{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/albert/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import keras\n",
    "import h5py\n",
    "import numpy as np\n",
    "from Utility import Utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 layer LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in sonnets and sequence them into integer sequences via map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "util = Utility()\n",
    "sonnets = util.get_shakespeare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonnetString = []\n",
    "for i in range(len(sonnets)):\n",
    "    sA = sonnets[i+1]\n",
    "    sonnetString.append(\"\")\n",
    "    for k in sA:\n",
    "        for j in k:\n",
    "            sonnetString[-1] += j + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LUT\n",
    "int_to_char = {0: ' ', 1: \"'\", 2: ',', 3: '-', 4: 'a', 5: 'b', 6: 'c', 7: 'd', 8: 'e', 9: 'f', 10: 'g', 11: 'h', 12: 'i', 13: 'k', 14: 'l', 15: 'm', 16: 'n', 17: 'o', 18: 'p', 19: 'q', 20: 'r', 21: 's', 22: 't', 23: 'u', 24: 'v', 25: 'w', 26: 'y', 27: 'j', 28: 'x', 29: 'z'}\n",
    "char_to_int = {' ': 0, \"'\": 1, ',': 2, '-': 3, 'a': 4, 'b': 5, 'c': 6, 'd': 7, 'e': 8, 'f': 9, 'g': 10, 'h': 11, 'i': 12, 'k': 13, 'l': 14, 'm': 15, 'n': 16, 'o': 17, 'p': 18, 'q': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'y': 26, 'j':27, 'x':28, 'z':29}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 40\n",
    "X = []\n",
    "Y = []\n",
    "dataX_seq = []\n",
    "for iter in range(len(sonnetString)):\n",
    "    text = sonnetString[iter]\n",
    "    seq = text[0:window]\n",
    "    dataX_seq.append([char_to_int[char] for char in seq])\n",
    "    for i in range(0, len(text) - window, 1):\n",
    "        seq = text[i:i + window]\n",
    "        out = text[i + window]\n",
    "        X.append([char_to_int[char] for char in seq])\n",
    "        Y.append(char_to_int[out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.reshape(X, (len(X), window, 1))\n",
    "Y = np_utils.to_categorical(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make model and train (takes forever...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(300, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(400, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(300))\n",
    "model.add(Dense(Y.shape[1], activation='softmax'))\n",
    "adam = keras.optimizers.Adam(lr = 1e-4)\n",
    "model.load_weights('weights-improvement-512230.hdf5')\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam)\n",
    "#filename = \"weights-improvement-61-1.4022.hdf5\"\n",
    "#model.load_weights(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from some guy on stackoverflow - calculates how much vram the model needs\n",
    "\n",
    "def get_model_memory_usage(batch_size, model):\n",
    "    import numpy as np\n",
    "    from keras import backend as K\n",
    "\n",
    "    shapes_mem_count = 0\n",
    "    for l in model.layers:\n",
    "        single_layer_mem = 1\n",
    "        for s in l.output_shape:\n",
    "            if s is None:\n",
    "                continue\n",
    "            single_layer_mem *= s\n",
    "        shapes_mem_count += single_layer_mem\n",
    "\n",
    "    trainable_count = np.sum([K.count_params(p) for p in set(model.trainable_weights)])\n",
    "    non_trainable_count = np.sum([K.count_params(p) for p in set(model.non_trainable_weights)])\n",
    "\n",
    "    total_memory = 4.0*batch_size*(shapes_mem_count + trainable_count + non_trainable_count)\n",
    "    gbytes = np.round(total_memory / (1024.0 ** 3), 3)\n",
    "    return gbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_memory_usage(400, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "86726/86726 [==============================] - 51s 586us/step - loss: 0.4589\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.45893, saving model to singlelayer2-01.hdf5\n",
      "Epoch 2/100\n",
      "86726/86726 [==============================] - 50s 576us/step - loss: 0.4611\n",
      "\n",
      "Epoch 00002: loss did not improve\n",
      "Epoch 3/100\n",
      "86726/86726 [==============================] - 51s 587us/step - loss: 0.4584\n",
      "\n",
      "Epoch 00003: loss improved from 0.45893 to 0.45839, saving model to singlelayer2-03.hdf5\n",
      "Epoch 4/100\n",
      "86726/86726 [==============================] - 50s 575us/step - loss: 0.4557\n",
      "\n",
      "Epoch 00004: loss improved from 0.45839 to 0.45572, saving model to singlelayer2-04.hdf5\n",
      "Epoch 5/100\n",
      "86726/86726 [==============================] - 49s 560us/step - loss: 0.4555\n",
      "\n",
      "Epoch 00005: loss improved from 0.45572 to 0.45552, saving model to singlelayer2-05.hdf5\n",
      "Epoch 6/100\n",
      "86726/86726 [==============================] - 48s 557us/step - loss: 0.4585\n",
      "\n",
      "Epoch 00006: loss did not improve\n",
      "Epoch 7/100\n",
      "86726/86726 [==============================] - 48s 557us/step - loss: 0.4555\n",
      "\n",
      "Epoch 00007: loss improved from 0.45552 to 0.45550, saving model to singlelayer2-07.hdf5\n",
      "Epoch 8/100\n",
      "86726/86726 [==============================] - 48s 559us/step - loss: 0.4513\n",
      "\n",
      "Epoch 00008: loss improved from 0.45550 to 0.45132, saving model to singlelayer2-08.hdf5\n",
      "Epoch 9/100\n",
      "86726/86726 [==============================] - 48s 558us/step - loss: 0.4502\n",
      "\n",
      "Epoch 00009: loss improved from 0.45132 to 0.45021, saving model to singlelayer2-09.hdf5\n",
      "Epoch 10/100\n",
      "86726/86726 [==============================] - 48s 558us/step - loss: 0.4477\n",
      "\n",
      "Epoch 00010: loss improved from 0.45021 to 0.44770, saving model to singlelayer2-10.hdf5\n",
      "Epoch 11/100\n",
      "86726/86726 [==============================] - 48s 557us/step - loss: 0.4474\n",
      "\n",
      "Epoch 00011: loss improved from 0.44770 to 0.44735, saving model to singlelayer2-11.hdf5\n",
      "Epoch 12/100\n",
      "86726/86726 [==============================] - 48s 558us/step - loss: 0.4458\n",
      "\n",
      "Epoch 00012: loss improved from 0.44735 to 0.44580, saving model to singlelayer2-12.hdf5\n",
      "Epoch 13/100\n",
      "86726/86726 [==============================] - 48s 557us/step - loss: 0.4443\n",
      "\n",
      "Epoch 00013: loss improved from 0.44580 to 0.44430, saving model to singlelayer2-13.hdf5\n",
      "Epoch 14/100\n",
      "86726/86726 [==============================] - 50s 572us/step - loss: 0.4401\n",
      "\n",
      "Epoch 00014: loss improved from 0.44430 to 0.44008, saving model to singlelayer2-14.hdf5\n",
      "Epoch 15/100\n",
      "86726/86726 [==============================] - 50s 581us/step - loss: 0.4428\n",
      "\n",
      "Epoch 00015: loss did not improve\n",
      "Epoch 16/100\n",
      "86726/86726 [==============================] - 52s 600us/step - loss: 0.4433\n",
      "\n",
      "Epoch 00016: loss did not improve\n",
      "Epoch 17/100\n",
      "86726/86726 [==============================] - 49s 564us/step - loss: 0.4416\n",
      "\n",
      "Epoch 00017: loss did not improve\n",
      "Epoch 18/100\n",
      "86726/86726 [==============================] - 48s 556us/step - loss: 0.4412\n",
      "\n",
      "Epoch 00018: loss did not improve\n",
      "Epoch 19/100\n",
      "86726/86726 [==============================] - 50s 575us/step - loss: 0.4375\n",
      "\n",
      "Epoch 00019: loss improved from 0.44008 to 0.43749, saving model to singlelayer2-19.hdf5\n",
      "Epoch 20/100\n",
      "86726/86726 [==============================] - 51s 589us/step - loss: 0.4403\n",
      "\n",
      "Epoch 00020: loss did not improve\n",
      "Epoch 21/100\n",
      "86726/86726 [==============================] - 51s 590us/step - loss: 0.4371\n",
      "\n",
      "Epoch 00021: loss improved from 0.43749 to 0.43710, saving model to singlelayer2-21.hdf5\n",
      "Epoch 22/100\n",
      "86726/86726 [==============================] - 51s 593us/step - loss: 0.4342\n",
      "\n",
      "Epoch 00022: loss improved from 0.43710 to 0.43422, saving model to singlelayer2-22.hdf5\n",
      "Epoch 23/100\n",
      "86726/86726 [==============================] - 51s 592us/step - loss: 0.4371\n",
      "\n",
      "Epoch 00023: loss did not improve\n",
      "Epoch 24/100\n",
      "86726/86726 [==============================] - 51s 587us/step - loss: 0.4344\n",
      "\n",
      "Epoch 00024: loss did not improve\n",
      "Epoch 25/100\n",
      "86726/86726 [==============================] - 51s 588us/step - loss: 0.4334\n",
      "\n",
      "Epoch 00025: loss improved from 0.43422 to 0.43338, saving model to singlelayer2-25.hdf5\n",
      "Epoch 26/100\n",
      "86726/86726 [==============================] - 51s 585us/step - loss: 0.4289\n",
      "\n",
      "Epoch 00026: loss improved from 0.43338 to 0.42885, saving model to singlelayer2-26.hdf5\n",
      "Epoch 27/100\n",
      "86726/86726 [==============================] - 51s 588us/step - loss: 0.4298\n",
      "\n",
      "Epoch 00027: loss did not improve\n",
      "Epoch 28/100\n",
      "86726/86726 [==============================] - 51s 587us/step - loss: 0.4296\n",
      "\n",
      "Epoch 00028: loss did not improve\n",
      "Epoch 29/100\n",
      "86726/86726 [==============================] - 51s 588us/step - loss: 0.4306\n",
      "\n",
      "Epoch 00029: loss did not improve\n",
      "Epoch 30/100\n",
      "86726/86726 [==============================] - 51s 588us/step - loss: 0.4269\n",
      "\n",
      "Epoch 00030: loss improved from 0.42885 to 0.42694, saving model to singlelayer2-30.hdf5\n",
      "Epoch 31/100\n",
      "86726/86726 [==============================] - 51s 586us/step - loss: 0.4301\n",
      "\n",
      "Epoch 00031: loss did not improve\n",
      "Epoch 32/100\n",
      "86726/86726 [==============================] - 51s 587us/step - loss: 0.4272\n",
      "\n",
      "Epoch 00032: loss did not improve\n",
      "Epoch 33/100\n",
      "86726/86726 [==============================] - 51s 586us/step - loss: 0.4289\n",
      "\n",
      "Epoch 00033: loss did not improve\n",
      "Epoch 34/100\n",
      "86726/86726 [==============================] - 51s 587us/step - loss: 0.4263\n",
      "\n",
      "Epoch 00034: loss improved from 0.42694 to 0.42632, saving model to singlelayer2-34.hdf5\n",
      "Epoch 35/100\n",
      "86726/86726 [==============================] - 51s 588us/step - loss: 0.4249\n",
      "\n",
      "Epoch 00035: loss improved from 0.42632 to 0.42487, saving model to singlelayer2-35.hdf5\n",
      "Epoch 36/100\n",
      "86726/86726 [==============================] - 51s 588us/step - loss: 0.4242\n",
      "\n",
      "Epoch 00036: loss improved from 0.42487 to 0.42416, saving model to singlelayer2-36.hdf5\n",
      "Epoch 37/100\n",
      "86726/86726 [==============================] - 51s 589us/step - loss: 0.4216\n",
      "\n",
      "Epoch 00037: loss improved from 0.42416 to 0.42156, saving model to singlelayer2-37.hdf5\n",
      "Epoch 38/100\n",
      "86726/86726 [==============================] - 51s 586us/step - loss: 0.4200\n",
      "\n",
      "Epoch 00038: loss improved from 0.42156 to 0.41999, saving model to singlelayer2-38.hdf5\n",
      "Epoch 39/100\n",
      "86726/86726 [==============================] - 53s 606us/step - loss: 0.4181\n",
      "\n",
      "Epoch 00039: loss improved from 0.41999 to 0.41810, saving model to singlelayer2-39.hdf5\n",
      "Epoch 40/100\n",
      "86726/86726 [==============================] - 51s 588us/step - loss: 0.4243\n",
      "\n",
      "Epoch 00040: loss did not improve\n",
      "Epoch 41/100\n",
      "86726/86726 [==============================] - 50s 577us/step - loss: 0.4217\n",
      "\n",
      "Epoch 00041: loss did not improve\n",
      "Epoch 42/100\n",
      "86726/86726 [==============================] - 49s 562us/step - loss: 0.4188\n",
      "\n",
      "Epoch 00042: loss did not improve\n",
      "Epoch 43/100\n",
      "86726/86726 [==============================] - 48s 559us/step - loss: 0.4168\n",
      "\n",
      "Epoch 00043: loss improved from 0.41810 to 0.41682, saving model to singlelayer2-43.hdf5\n",
      "Epoch 44/100\n",
      "86726/86726 [==============================] - 49s 560us/step - loss: 0.4186\n",
      "\n",
      "Epoch 00044: loss did not improve\n",
      "Epoch 45/100\n",
      "86726/86726 [==============================] - 49s 561us/step - loss: 0.4163\n",
      "\n",
      "Epoch 00045: loss improved from 0.41682 to 0.41628, saving model to singlelayer2-45.hdf5\n",
      "Epoch 46/100\n",
      "86726/86726 [==============================] - 49s 561us/step - loss: 0.4112\n",
      "\n",
      "Epoch 00046: loss improved from 0.41628 to 0.41121, saving model to singlelayer2-46.hdf5\n",
      "Epoch 47/100\n",
      "86726/86726 [==============================] - 48s 558us/step - loss: 0.4135\n",
      "\n",
      "Epoch 00047: loss did not improve\n",
      "Epoch 48/100\n",
      "86726/86726 [==============================] - 51s 584us/step - loss: 0.4108\n",
      "\n",
      "Epoch 00048: loss improved from 0.41121 to 0.41083, saving model to singlelayer2-48.hdf5\n",
      "Epoch 49/100\n",
      "86726/86726 [==============================] - 52s 599us/step - loss: 0.4095\n",
      "\n",
      "Epoch 00049: loss improved from 0.41083 to 0.40954, saving model to singlelayer2-49.hdf5\n",
      "Epoch 50/100\n",
      "86726/86726 [==============================] - 51s 589us/step - loss: 0.4116\n",
      "\n",
      "Epoch 00050: loss did not improve\n",
      "Epoch 51/100\n",
      "86726/86726 [==============================] - 54s 623us/step - loss: 0.4118\n",
      "\n",
      "Epoch 00051: loss did not improve\n",
      "Epoch 52/100\n",
      "86726/86726 [==============================] - 52s 596us/step - loss: 0.4087\n",
      "\n",
      "Epoch 00052: loss improved from 0.40954 to 0.40872, saving model to singlelayer2-52.hdf5\n",
      "Epoch 53/100\n",
      "86726/86726 [==============================] - 51s 586us/step - loss: 0.4104\n",
      "\n",
      "Epoch 00053: loss did not improve\n",
      "Epoch 54/100\n",
      "86726/86726 [==============================] - 51s 587us/step - loss: 0.4079\n",
      "\n",
      "Epoch 00054: loss improved from 0.40872 to 0.40794, saving model to singlelayer2-54.hdf5\n",
      "Epoch 55/100\n",
      " 7500/86726 [=>............................] - ETA: 48s - loss: 0.4052"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-54a1cfa699ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcallbacks_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1710\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#checkpoint code taken from public tutorial\n",
    "filepath=\"singlelayer2-{epoch:02d}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "model.fit(X, Y, epochs=100, batch_size=300, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample function from keras example code (see keras official github)\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds+0.001) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    if(np.sum(exp_preds) != 0):\n",
    "        preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = '  shall i compare thee to a summer\\'s day'\n",
    "seedArr = []\n",
    "for i in range(len(seed)):\n",
    "    seedArr.append(char_to_int[seed[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  shall i compare thee to a summer's day\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed that is the actual start of a sonnet\n",
    "start = np.random.randint(0, len(dataX_seq)-1)\n",
    "pattern = dataX_seq[start]\n",
    "pattern = seedArr\n",
    "print(''.join([int_to_char[nt] for nt in pattern]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trength of love, with in the eyes of thee this i prognostinate thee that thou art not, though i feel thou wealth she had in this, my lovd no whit disdarte, living on a living brow before the golden tresses of the dead, the right of sepulchres, were shorn away, to live a second life on second head, ere beauty's dead fleece made another gay in him those holy antique hours are seen, without all ornament, it self and true, making no summer of another's green, robbing no old to dress his beauty new, and him as for a map doth nature store, to show false art what beauty was of yore being ahe the ceasth's eear from my love's time these would i say my self the paintered seem from thee, thou shall have profaned all wormd with travels the cloud thou break, to dry the rain on my storm-beaten face, for no man well of such a salve can speak, that heals the wound, and cures not the disgrace nor can thy shame give physic to my grief, though thou repent, yet i have still the loss, th' offender's sorrow lends but weak relief to him that bears the strong offence's cross ah but those tears are meant to lose thee, my loss is my love's fair a hand, as roon as yell beseem thy heart to mourn for me since mourning doth thee grace, and suit thy pity like in every part then will i swear beauty herself is black, and all they foul that thy complexion lack sunmer of wour mamed in some part, nine is for her "
     ]
    }
   ],
   "source": [
    "# generate characters\n",
    "nchar = 1400\n",
    "for i in range(nchar):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    prediction = model.predict(x, verbose=0)[0]\n",
    "    index = sample(prediction, 0.25)\n",
    "    \n",
    "    result = int_to_char[index]\n",
    "    print(result, end='')\n",
    "    \n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
